"""
This type stub file was generated by pyright.
"""

import ctranslate2
import numpy as np
from dataclasses import dataclass
from typing import BinaryIO, Iterable, List, Optional, Tuple, Union
from faster_whisper.tokenizer import Tokenizer
from faster_whisper.vad import VadOptions

@dataclass
class Word:
    start: float
    end: float
    word: str
    probability: float
    ...


@dataclass
class Segment:
    id: int
    seek: int
    start: float
    end: float
    text: str
    tokens: List[int]
    avg_logprob: float
    compression_ratio: float
    no_speech_prob: float
    words: Optional[List[Word]]
    temperature: Optional[float]
    ...


@dataclass
class TranscriptionOptions:
    beam_size: int
    best_of: int
    patience: float
    length_penalty: float
    repetition_penalty: float
    no_repeat_ngram_size: int
    log_prob_threshold: Optional[float]
    no_speech_threshold: Optional[float]
    compression_ratio_threshold: Optional[float]
    condition_on_previous_text: bool
    prompt_reset_on_temperature: float
    temperatures: List[float]
    initial_prompt: Optional[Union[str, Iterable[int]]]
    prefix: Optional[str]
    suppress_blank: bool
    suppress_tokens: Optional[List[int]]
    without_timestamps: bool
    max_initial_timestamp: float
    word_timestamps: bool
    prepend_punctuations: str
    append_punctuations: str
    multilingual: bool
    max_new_tokens: Optional[int]
    clip_timestamps: Union[str, List[float]]
    hallucination_silence_threshold: Optional[float]
    hotwords: Optional[str]
    ...


@dataclass
class TranscriptionInfo:
    language: str
    language_probability: float
    duration: float
    duration_after_vad: float
    all_language_probs: Optional[List[Tuple[str, float]]]
    transcription_options: TranscriptionOptions
    vad_options: VadOptions
    ...


class BatchedInferencePipeline:
    def __init__(self, model) -> None:
        ...
    
    def forward(self, features, tokenizer, chunks_metadata, options): # -> list[Any]:
        ...
    
    def generate_segment_batched(self, features: np.ndarray, tokenizer: Tokenizer, options: TranscriptionOptions): # -> tuple[Any, list[Any]]:
        ...
    
    def transcribe(self, audio: Union[str, BinaryIO, np.ndarray], language: Optional[str] = ..., task: str = ..., log_progress: bool = ..., beam_size: int = ..., best_of: int = ..., patience: float = ..., length_penalty: float = ..., repetition_penalty: float = ..., no_repeat_ngram_size: int = ..., temperature: Union[float, List[float], Tuple[float, ...]] = ..., compression_ratio_threshold: Optional[float] = ..., log_prob_threshold: Optional[float] = ..., no_speech_threshold: Optional[float] = ..., condition_on_previous_text: bool = ..., prompt_reset_on_temperature: float = ..., initial_prompt: Optional[Union[str, Iterable[int]]] = ..., prefix: Optional[str] = ..., suppress_blank: bool = ..., suppress_tokens: Optional[List[int]] = ..., without_timestamps: bool = ..., max_initial_timestamp: float = ..., word_timestamps: bool = ..., prepend_punctuations: str = ..., append_punctuations: str = ..., multilingual: bool = ..., vad_filter: bool = ..., vad_parameters: Optional[Union[dict, VadOptions]] = ..., max_new_tokens: Optional[int] = ..., chunk_length: Optional[int] = ..., clip_timestamps: Optional[List[dict]] = ..., hallucination_silence_threshold: Optional[float] = ..., batch_size: int = ..., hotwords: Optional[str] = ..., language_detection_threshold: Optional[float] = ..., language_detection_segments: int = ...) -> Tuple[Iterable[Segment], TranscriptionInfo]:
        """transcribe audio in chunks in batched fashion and return with language info.

        Arguments:
            audio: Path to the input file (or a file-like object), or the audio waveform.
            language: The language spoken in the audio. It should be a language code such
                as "en" or "fr". If not set, the language will be detected in the first 30 seconds
                of audio.
            task: Task to execute (transcribe or translate).
            log_progress: whether to show progress bar or not.
            beam_size: Beam size to use for decoding.
            best_of: Number of candidates when sampling with non-zero temperature.
            patience: Beam search patience factor.
            length_penalty: Exponential length penalty constant.
            repetition_penalty: Penalty applied to the score of previously generated tokens
                (set > 1 to penalize).
            no_repeat_ngram_size: Prevent repetitions of ngrams with this size (set 0 to disable).
            temperature: Temperature for sampling. If a list or tuple is passed,
                only the first value is used.
            initial_prompt: Optional text string or iterable of token ids to provide as a
                prompt for the each window.
            suppress_blank: Suppress blank outputs at the beginning of the sampling.
            suppress_tokens: List of token IDs to suppress. -1 will suppress a default set
                of symbols as defined in `tokenizer.non_speech_tokens()`.
            without_timestamps: Only sample text tokens.
            word_timestamps: Extract word-level timestamps using the cross-attention pattern
                and dynamic time warping, and include the timestamps for each word in each segment.
                Set as False.
            prepend_punctuations: If word_timestamps is True, merge these punctuation symbols
                with the next word
            append_punctuations: If word_timestamps is True, merge these punctuation symbols
                with the previous word
            multilingual: Perform language detection on every segment.
            vad_filter: Enable the voice activity detection (VAD) to filter out parts of the audio
                without speech. This step is using the Silero VAD model
                https://github.com/snakers4/silero-vad.
            vad_parameters: Dictionary of Silero VAD parameters or VadOptions class (see available
                parameters and default values in the class `VadOptions`).
            max_new_tokens: Maximum number of new tokens to generate per-chunk. If not set,
                the maximum will be set by the default max_length.
            chunk_length: The length of audio segments. If it is not None, it will overwrite the
                default chunk_length of the FeatureExtractor.
            clip_timestamps: Optionally provide list of dictionaries each containing "start" and
                "end" keys that specify the start and end of the voiced region within
                `chunk_length` boundary. vad_filter will be ignored if clip_timestamps is used.
            batch_size: the maximum number of parallel requests to model for decoding.
            hotwords:
                Hotwords/hint phrases to the model. Has no effect if prefix is not None.
            language_detection_threshold: If the maximum probability of the language tokens is
                higher than this value, the language is detected.
            language_detection_segments: Number of segments to consider for the language detection.

        Unused Arguments
            compression_ratio_threshold: If the gzip compression ratio is above this value,
                treat as failed.
            log_prob_threshold: If the average log probability over sampled tokens is
                below this value, treat as failed.
            no_speech_threshold: If the no_speech probability is higher than this value AND
                the average log probability over sampled tokens is below `log_prob_threshold`,
                consider the segment as silent.
            condition_on_previous_text: If True, the previous output of the model is provided
                as a prompt for the next window; disabling may make the text inconsistent across
                windows, but the model becomes less prone to getting stuck in a failure loop,
                such as repetition looping or timestamps going out of sync. Set as False
            prompt_reset_on_temperature: Resets prompt if temperature is above this value.
                Arg has effect only if condition_on_previous_text is True. Set at 0.5
            prefix: Optional text to provide as a prefix at the beginning of each window.
            max_initial_timestamp: The initial timestamp cannot be later than this, set at 0.0.
            hallucination_silence_threshold: Optional[float]
                When word_timestamps is True, skip silent periods longer than this threshold
                (in seconds) when a possible hallucination is detected. set as None.
        Returns:
          A tuple with:

            - a generator over transcribed segments
            - an instance of TranscriptionInfo
        """
        ...
    


class WhisperModel:
    def __init__(self, model_size_or_path: str, device: str = ..., device_index: Union[int, List[int]] = ..., compute_type: str = ..., cpu_threads: int = ..., num_workers: int = ..., download_root: Optional[str] = ..., local_files_only: bool = ..., files: dict = ..., revision: Optional[str] = ..., use_auth_token: Optional[Union[str, bool]] = ..., **model_kwargs) -> None:
        """Initializes the Whisper model.

        Args:
          model_size_or_path: Size of the model to use (tiny, tiny.en, base, base.en,
            small, small.en, distil-small.en, medium, medium.en, distil-medium.en, large-v1,
            large-v2, large-v3, large, distil-large-v2, distil-large-v3, large-v3-turbo, or turbo),
            a path to a converted model directory, or a CTranslate2-converted Whisper model ID from
            the HF Hub. When a size or a model ID is configured, the converted model is downloaded
            from the Hugging Face Hub.
          device: Device to use for computation ("cpu", "cuda", "auto").
          device_index: Device ID to use.
            The model can also be loaded on multiple GPUs by passing a list of IDs
            (e.g. [0, 1, 2, 3]). In that case, multiple transcriptions can run in parallel
            when transcribe() is called from multiple Python threads (see also num_workers).
          compute_type: Type to use for computation.
            See https://opennmt.net/CTranslate2/quantization.html.
          cpu_threads: Number of threads to use when running on CPU (4 by default).
            A non zero value overrides the OMP_NUM_THREADS environment variable.
          num_workers: When transcribe() is called from multiple Python threads,
            having multiple workers enables true parallelism when running the model
            (concurrent calls to self.model.generate() will run in parallel).
            This can improve the global throughput at the cost of increased memory usage.
          download_root: Directory where the models should be saved. If not set, the models
            are saved in the standard Hugging Face cache directory.
          local_files_only:  If True, avoid downloading the file and return the path to the
            local cached file if it exists.
          files: Load model files from the memory. This argument is a dictionary mapping file names
            to file contents as file-like or bytes objects. If this is set, model_path acts as an
            identifier for this model.
          revision:
            An optional Git revision id which can be a branch name, a tag, or a
            commit hash.
          use_auth_token: HuggingFace authentication token or True to use the
            token stored by the HuggingFace config folder.
        """
        ...
    
    @property
    def supported_languages(self) -> List[str]:
        """The languages supported by the model."""
        ...
    
    def transcribe(self, audio: Union[str, BinaryIO, np.ndarray], language: Optional[str] = ..., task: str = ..., log_progress: bool = ..., beam_size: int = ..., best_of: int = ..., patience: float = ..., length_penalty: float = ..., repetition_penalty: float = ..., no_repeat_ngram_size: int = ..., temperature: Union[float, List[float], Tuple[float, ...]] = ..., compression_ratio_threshold: Optional[float] = ..., log_prob_threshold: Optional[float] = ..., no_speech_threshold: Optional[float] = ..., condition_on_previous_text: bool = ..., prompt_reset_on_temperature: float = ..., initial_prompt: Optional[Union[str, Iterable[int]]] = ..., prefix: Optional[str] = ..., suppress_blank: bool = ..., suppress_tokens: Optional[List[int]] = ..., without_timestamps: bool = ..., max_initial_timestamp: float = ..., word_timestamps: bool = ..., prepend_punctuations: str = ..., append_punctuations: str = ..., multilingual: bool = ..., vad_filter: bool = ..., vad_parameters: Optional[Union[dict, VadOptions]] = ..., max_new_tokens: Optional[int] = ..., chunk_length: Optional[int] = ..., clip_timestamps: Union[str, List[float]] = ..., hallucination_silence_threshold: Optional[float] = ..., hotwords: Optional[str] = ..., language_detection_threshold: Optional[float] = ..., language_detection_segments: int = ...) -> Tuple[Iterable[Segment], TranscriptionInfo]:
        """Transcribes an input file.

        Arguments:
          audio: Path to the input file (or a file-like object), or the audio waveform.
          language: The language spoken in the audio. It should be a language code such
            as "en" or "fr". If not set, the language will be detected in the first 30 seconds
            of audio.
          task: Task to execute (transcribe or translate).
          log_progress: whether to show progress bar or not.
          beam_size: Beam size to use for decoding.
          best_of: Number of candidates when sampling with non-zero temperature.
          patience: Beam search patience factor.
          length_penalty: Exponential length penalty constant.
          repetition_penalty: Penalty applied to the score of previously generated tokens
            (set > 1 to penalize).
          no_repeat_ngram_size: Prevent repetitions of ngrams with this size (set 0 to disable).
          temperature: Temperature for sampling. It can be a tuple of temperatures,
            which will be successively used upon failures according to either
            `compression_ratio_threshold` or `log_prob_threshold`.
          compression_ratio_threshold: If the gzip compression ratio is above this value,
            treat as failed.
          log_prob_threshold: If the average log probability over sampled tokens is
            below this value, treat as failed.
          no_speech_threshold: If the no_speech probability is higher than this value AND
            the average log probability over sampled tokens is below `log_prob_threshold`,
            consider the segment as silent.
          condition_on_previous_text: If True, the previous output of the model is provided
            as a prompt for the next window; disabling may make the text inconsistent across
            windows, but the model becomes less prone to getting stuck in a failure loop,
            such as repetition looping or timestamps going out of sync.
          prompt_reset_on_temperature: Resets prompt if temperature is above this value.
            Arg has effect only if condition_on_previous_text is True.
          initial_prompt: Optional text string or iterable of token ids to provide as a
            prompt for the first window.
          prefix: Optional text to provide as a prefix for the first window.
          suppress_blank: Suppress blank outputs at the beginning of the sampling.
          suppress_tokens: List of token IDs to suppress. -1 will suppress a default set
            of symbols as defined in `tokenizer.non_speech_tokens()`.
          without_timestamps: Only sample text tokens.
          max_initial_timestamp: The initial timestamp cannot be later than this.
          word_timestamps: Extract word-level timestamps using the cross-attention pattern
            and dynamic time warping, and include the timestamps for each word in each segment.
          prepend_punctuations: If word_timestamps is True, merge these punctuation symbols
            with the next word
          append_punctuations: If word_timestamps is True, merge these punctuation symbols
            with the previous word
          multilingual: Perform language detection on every segment.
          vad_filter: Enable the voice activity detection (VAD) to filter out parts of the audio
            without speech. This step is using the Silero VAD model
            https://github.com/snakers4/silero-vad.
          vad_parameters: Dictionary of Silero VAD parameters or VadOptions class (see available
            parameters and default values in the class `VadOptions`).
          max_new_tokens: Maximum number of new tokens to generate per-chunk. If not set,
            the maximum will be set by the default max_length.
          chunk_length: The length of audio segments. If it is not None, it will overwrite the
            default chunk_length of the FeatureExtractor.
          clip_timestamps:
            Comma-separated list start,end,start,end,... timestamps (in seconds) of clips to
             process. The last end timestamp defaults to the end of the file.
             vad_filter will be ignored if clip_timestamps is used.
          hallucination_silence_threshold:
            When word_timestamps is True, skip silent periods longer than this threshold
             (in seconds) when a possible hallucination is detected
          hotwords:
            Hotwords/hint phrases to provide the model with. Has no effect if prefix is not None.
          language_detection_threshold: If the maximum probability of the language tokens is higher
           than this value, the language is detected.
          language_detection_segments: Number of segments to consider for the language detection.
        Returns:
          A tuple with:

            - a generator over transcribed segments
            - an instance of TranscriptionInfo
        """
        ...
    
    def generate_segments(self, features: np.ndarray, tokenizer: Tokenizer, options: TranscriptionOptions, log_progress, encoder_output: Optional[ctranslate2.StorageView] = ...) -> Iterable[Segment]:
        ...
    
    def encode(self, features: np.ndarray) -> ctranslate2.StorageView:
        ...
    
    def generate_with_fallback(self, encoder_output: ctranslate2.StorageView, prompt: List[int], tokenizer: Tokenizer, options: TranscriptionOptions) -> Tuple[ctranslate2.models.WhisperGenerationResult, float, float, float]:
        ...
    
    def get_prompt(self, tokenizer: Tokenizer, previous_tokens: List[int], without_timestamps: bool = ..., prefix: Optional[str] = ..., hotwords: Optional[str] = ...) -> List[int]:
        ...
    
    def add_word_timestamps(self, segments: List[dict], tokenizer: Tokenizer, encoder_output: ctranslate2.StorageView, num_frames: int, prepend_punctuations: str, append_punctuations: str, last_speech_timestamp: float) -> float:
        ...
    
    def find_alignment(self, tokenizer: Tokenizer, text_tokens: List[int], encoder_output: ctranslate2.StorageView, num_frames: int, median_filter_width: int = ...) -> List[dict]:
        ...
    
    def detect_language(self, audio: Optional[np.ndarray] = ..., features: Optional[np.ndarray] = ..., vad_filter: bool = ..., vad_parameters: Union[dict, VadOptions] = ..., language_detection_segments: int = ..., language_detection_threshold: float = ...) -> Tuple[str, float, List[Tuple[str, float]]]:
        """
        Use Whisper to detect the language of the input audio or features.

        Arguments:
            audio: Input audio signal, must be a 1D float array sampled at 16khz.
            features: Input Mel spectrogram features, must be a float array with
                shape (n_mels, n_frames), if `audio` is provided, the features will be ignored.
                Either `audio` or `features` must be provided.
            vad_filter: Enable the voice activity detection (VAD) to filter out parts of the audio
                without speech. This step is using the Silero VAD model.
            vad_parameters: Dictionary of Silero VAD parameters or VadOptions class (see available
                parameters and default values in the class `VadOptions`).
            language_detection_threshold: If the maximum probability of the language tokens is
                higher than this value, the language is detected.
            language_detection_segments: Number of segments to consider for the language detection.

        Returns:
            language: Detected language.
            languege_probability: Probability of the detected language.
            all_language_probs: List of tuples with all language names and probabilities.
        """
        ...
    


def restore_speech_timestamps(segments: Iterable[Segment], speech_chunks: List[dict], sampling_rate: int) -> Iterable[Segment]:
    ...

def get_ctranslate2_storage(segment: np.ndarray) -> ctranslate2.StorageView:
    ...

def get_compression_ratio(text: str) -> float:
    ...

def get_suppressed_tokens(tokenizer: Tokenizer, suppress_tokens: Tuple[int]) -> Optional[List[int]]:
    ...

def merge_punctuations(alignment: List[dict], prepended: str, appended: str) -> None:
    ...

