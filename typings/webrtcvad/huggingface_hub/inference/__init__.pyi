"""
This type stub file was generated by pyright.
"""

import base64
import io
import json
import logging
import mimetypes
from contextlib import contextmanager
from dataclasses import dataclass
from pathlib import Path
from typing import Any, AsyncIterable, BinaryIO, ContextManager, Dict, Generator, Iterable, List, Literal, NoReturn, Optional, TYPE_CHECKING, Union, overload
from requests import HTTPError
from huggingface_hub.errors import GenerationError, IncompleteGenerationError, OverloadedError, TextGenerationError, UnknownError, ValidationError
from ..utils import get_session, is_aiohttp_available, is_numpy_available, is_pillow_available
from ._generated.types import ChatCompletionStreamOutput, TextGenerationStreamOutput
from aiohttp import ClientResponse, ClientSession
from PIL.Image import Image

"""Contains utilities used by both the sync and async inference clients."""
if TYPE_CHECKING:
    ...
UrlT = str
PathT = Union[str, Path]
BinaryT = Union[bytes, BinaryIO]
ContentT = Union[BinaryT, PathT, UrlT, "Image"]
TASKS_EXPECTING_IMAGES = ...
logger = ...
@dataclass
class RequestParameters:
    url: str
    task: str
    model: Optional[str]
    json: Optional[Union[str, Dict, List]]
    data: Optional[ContentT]
    headers: Dict[str, Any]
    ...


@dataclass
class ModelStatus:
    """
    This Dataclass represents the model status in the HF Inference API.

    Args:
        loaded (`bool`):
            If the model is currently loaded into HF's Inference API. Models
            are loaded on-demand, leading to the user's first request taking longer.
            If a model is loaded, you can be assured that it is in a healthy state.
        state (`str`):
            The current state of the model. This can be 'Loaded', 'Loadable', 'TooBig'.
            If a model's state is 'Loadable', it's not too big and has a supported
            backend. Loadable models are automatically loaded when the user first
            requests inference on the endpoint. This means it is transparent for the
            user to load a model, except that the first call takes longer to complete.
        compute_type (`Dict`):
            Information about the compute resource the model is using or will use, such as 'gpu' type and number of
            replicas.
        framework (`str`):
            The name of the framework that the model was built with, such as 'transformers'
            or 'text-generation-inference'.
    """
    loaded: bool
    state: str
    compute_type: Dict
    framework: str
    ...


_UNSUPPORTED_TEXT_GENERATION_KWARGS: Dict[Optional[str], List[str]] = ...
def raise_text_generation_error(http_error: HTTPError) -> NoReturn:
    """
    Try to parse text-generation-inference error message and raise HTTPError in any case.

    Args:
        error (`HTTPError`):
            The HTTPError that have been raised.
    """
    ...

